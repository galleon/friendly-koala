# friendly-koala

# Introduction
## Who is the book for?
## What is the book for?


# Part 1 Setting-up the foundations

## Meeting PyTorch [GD]?

### Basic concepts 
### GPU speedup
### Creating a model
### Optimising a model
### Datasets and Dataloaders
### Summary

## Getting started with Colab
### What is Google Colab
### Accessing your datasets
### Your first notebook in Colab
### Summary

## Examples of real-world applications
### HuggingFace transformers
### Other models usages (TBC)

# Part 2 from recurrent networks to transformers

## Recurrent Neural networks [GD]
### Principles
### LSTM & GRU cells
### Architectures
### RNN Unfolding & bidirectional RNN
### Constraints and limitations
### Summary

## Attention networks [GD]
### Principles
### Training
### Summary

## Transformer 
### Encoder/decoder architecture
### Self-supervised training
### Multi-head attention
### Fine-tuning approach
### Multi-task learning
### Synthesis

# Part 3 Transformers in Action

## Using transformers for NLP [GD]
### Basic concepts 
### Key models and libraries - HuggingFace transformers 
### Training for end-tasks
### Summary
## Using transformers for computer vision
### Basic concepts 
### Key models and libraries
### Training for end-tasks
### Summary

## Using transformers for time series
### Basic concepts 
### Key models and libraries
### Training for end-tasks
### Summary

## Using transformers for decision making [GD]?
### Basic concepts 
### Key models and libraries
### Training for end-tasks
### Summary

## Beyond Vanilla Transformer [GD]
### Transformer evolution: future research paths
Interpretation of models
Bigger models
Smaller models
### Major research papers

